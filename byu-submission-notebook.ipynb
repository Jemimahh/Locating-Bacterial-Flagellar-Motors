{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91249,"databundleVersionId":11294684,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":286256943,"sourceType":"kernelVersion"},{"sourceId":44142,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":37071,"modelId":52045},{"sourceId":683524,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":518665,"modelId":533203}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":1473.740735,"end_time":"2025-03-07T10:48:57.252544","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-03-07T10:24:23.511809","version":"2.6.0"},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## import libraries","metadata":{"id":"c580cfbe-bf6c-4b70-89ce-d290c2ffd88b"}},{"cell_type":"code","source":"import sys, subprocess\n\n# reinstall scikit-learn so it matches the current numpy ABI\n# remove the problem packages completely\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"numpy\", \"matplotlib\"])\n\n# install versions that work together on Kaggle\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n                       \"numpy==2.0.2\",\n                       \"matplotlib==3.10.0\",\n                       \"protobuf<6\"])\n\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"scikit-learn\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-cache-dir\",\n                       \"scikit-learn==1.5.2\"])\n!pip install google-cloud-bigquery-storage>=2.30.0,<3.0.0\n# or to install a specific version\n!pip install google-cloud-bigquery-storage==2.30.0 \n\n\nprint(\"âœ… scikit-learn reinstalled. Now restart the session.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Extract the offline packages\n!tar xfvz /kaggle/input/ultralytics-for-offline-install/archive.tar.gz\n\n# 2. CRITICAL: Remove the numpy wheel so it doesn't overwrite the system version\n!rm packages/numpy*\n!rm packages/scipy*\n\n# 3. Install only ultralytics (using the system's safe numpy/scipy)\n!pip -q install --no-index --find-links=./packages ultralytics\n\n# 4. Clean up\n!rm -rf ./packages","metadata":{"trusted":true,"id":"6643dc10-6ae9-4c47-b162-c2be0a2fb7e8","executionInfo":{"status":"ok","timestamp":1765676714325,"user_tz":360,"elapsed":12855,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}},"outputId":"13c8cce6-533a-4fb4-ca00-a77b6d6be64a"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Processing & Analysis\nimport numpy as np  # Numerical computations\nimport pandas as pd  # Data manipulation & analysis\nfrom tqdm.notebook import tqdm  # Progress bar for loops\n\n# Machine Learning & Deep Learning\nimport torch  # PyTorch framework\nimport torch.nn as nn  # Neural network layers\nimport torch.nn.functional as F  # Activation functions\nimport torch.optim as optim  # Optimizers for training\nfrom torch.utils.data import Dataset, DataLoader  # Custom dataset handling\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau  # Learning rate scheduler\n\n# Computer Vision & Image Processing\nimport cv2  # OpenCV for image processing\nfrom PIL import Image, ImageDraw  # Pillow for handling images\nimport seaborn as sns  # Data visualization\nimport matplotlib.pyplot as plt  # Plotting library\nfrom matplotlib.patches import Rectangle  # Drawing bounding boxes\n\n# Dataset & File Handling\nimport yaml  # YAML file handling\nimport json  # JSON file handling\nimport os  # Operating system interactions\nimport glob  # File searching\n\n# Threading & Parallel Processing\nimport threading  # Multi-threading for performance improvement\nimport time  # Timing operations\nfrom contextlib import nullcontext  # Handling context management\nfrom concurrent.futures import ThreadPoolExecutor  # Thread-based parallelism\n\n# Deep Learning Models\nfrom ultralytics import YOLO  # YOLO model for object detection\n\n# Machine Learning Utilities\nfrom sklearn.model_selection import train_test_split  # Data splitting for training/validation\n\n# Visualization\nimport plotly.express as px  # Interactive visualizations\n\n# Mathematical Operations\nimport math  # Mathematical functions\nimport random  # Random number generation\n","metadata":{"_kg_hide-input":true,"papermill":{"duration":10.833904,"end_time":"2025-03-07T10:25:27.730720","exception":false,"start_time":"2025-03-07T10:25:16.896816","status":"completed"},"tags":[],"trusted":true,"id":"1ed498bd","executionInfo":{"status":"ok","timestamp":1765676734308,"user_tz":360,"elapsed":14262,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}},"outputId":"5497b470-5364-4ced-eabc-a5de6df7ef73"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## directories","metadata":{"id":"47cbd0d8-e74c-4d39-a8b3-6ccfb59db1d5"}},{"cell_type":"code","source":"Train_dir = \"/kaggle/input/byu-locating-bacterial-flagellar-motors-2025/train\"\nTest_dir = \"/kaggle/input/byu-locating-bacterial-flagellar-motors-2025/test\"","metadata":{"trusted":true,"id":"4bcae9d8-3437-4939-976b-becab8d4b609","executionInfo":{"status":"ok","timestamp":1765676739899,"user_tz":360,"elapsed":18,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Check device","metadata":{"id":"4a4a07e8-e486-4e91-a493-378ee19b4ad3"}},{"cell_type":"code","source":"# Create output directories if they don't exist\nos.makedirs('./', exist_ok=True)\nos.makedirs('./', exist_ok=True)\n\n# Set device: Use GPU if available; otherwise, fall back to CPU\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\n\n# Set random seeds for reproducibility\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(RANDOM_SEED)\n    torch.backends.cudnn.deterministic = True\n","metadata":{"papermill":{"duration":0.093398,"end_time":"2025-03-07T10:25:27.851806","exception":false,"start_time":"2025-03-07T10:25:27.758408","status":"completed"},"tags":[],"trusted":true,"id":"95c6684d","executionInfo":{"status":"ok","timestamp":1765676741900,"user_tz":360,"elapsed":31,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}},"outputId":"4064b62c-5a2d-4167-f974-bf87be5c8e8b"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## load the dataset","metadata":{"papermill":{"duration":0.008726,"end_time":"2025-03-07T10:25:27.889552","exception":false,"start_time":"2025-03-07T10:25:27.880826","status":"completed"},"tags":[],"id":"8835731f"}},{"cell_type":"code","source":"# Load the training labels CSV into a pandas DataFrame\ntrain_df = pd.read_csv(\"/kaggle/input/byu-locating-bacterial-flagellar-motors-2025/train_labels.csv\")\ntrain_df.head()","metadata":{"papermill":{"duration":0.069256,"end_time":"2025-03-07T10:25:27.967899","exception":false,"start_time":"2025-03-07T10:25:27.898643","status":"completed"},"tags":[],"trusted":true,"id":"2ed2d16e","executionInfo":{"status":"error","timestamp":1765676744007,"user_tz":360,"elapsed":48,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}},"outputId":"e26ddef6-e7ef-4131-c361-d28406de82ce"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Motor Stat","metadata":{"papermill":{"duration":0.00936,"end_time":"2025-03-07T10:25:27.986684","exception":false,"start_time":"2025-03-07T10:25:27.977324","status":"completed"},"tags":[],"id":"1d91061e"}},{"cell_type":"code","source":"# Create the bar plot\nax = train_df.groupby('tomo_id')['Number of motors'].first().value_counts().plot(kind=\"bar\", figsize=(10, 6), color=\"skyblue\")\n\n# Annotate each bar with its count value\nfor p in ax.patches:\n    ax.annotate(f'{p.get_height()}',\n                (p.get_x() + p.get_width() / 2, p.get_height()),  # Position (x, y)\n                ha='center', va='bottom',  # Center alignment\n                fontsize=12, fontweight='bold', color='black')  # Text styling\n\n# Labels and title\nax.set_xlabel(\"Number of Motors\", fontsize=14)\nax.set_ylabel(\"Count\", fontsize=14)\nax.set_title(\"Distribution of Motors per Tomogram\", fontsize=16)\nplt.xticks(rotation=0)  # Keep x-axis labels horizontal\n\n# Show plot\nplt.show()\n","metadata":{"trusted":true,"id":"c13d5fd0-6ea1-4306-bea3-d3e94c5f6fb7","executionInfo":{"status":"aborted","timestamp":1765676577790,"user_tz":360,"elapsed":36309,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tomogram_ranges = {\n    \"Z-axis (slices)\": (train_df[\"Array shape (axis 0)\"].min(), train_df[\"Array shape (axis 0)\"].max()),\n    \"X-axis (width)\":  (train_df[\"Array shape (axis 1)\"].min(), train_df[\"Array shape (axis 1)\"].max()),\n    \"Y-axis (height)\": (train_df[\"Array shape (axis 2)\"].min(), train_df[\"Array shape (axis 2)\"].max())\n}\n\n# Print formatted output\nprint(\"\\n **Tomogram Size Ranges**:\")\nfor axis, (min_val, max_val) in tomogram_ranges.items():\n    print(f\"{axis}: {min_val} to {max_val}\")","metadata":{"papermill":{"duration":0.037194,"end_time":"2025-03-07T10:25:28.391255","exception":false,"start_time":"2025-03-07T10:25:28.354061","status":"completed"},"tags":[],"trusted":true,"id":"ad505c3f","executionInfo":{"status":"aborted","timestamp":1765676577792,"user_tz":360,"elapsed":36306,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Relationships between motor axes and tomogram shapes","metadata":{"id":"5a2ba8bc-be79-4b9e-a875-187cf9d2e7c1"}},{"cell_type":"code","source":"# Extract data\nx = train_df['Motor axis 0']\ny = train_df['Motor axis 1']\nz = train_df['Motor axis 2']\ncolors = train_df['Number of motors']\n\n# Create 3D plot\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\n\n# Scatter plot\nsc = ax.scatter(x, y, z, c=colors, cmap=\"viridis\", s=50, alpha=0.85)\n\n# Color bar\ncbar = plt.colorbar(sc, ax=ax)\ncbar.set_label('Number of motors')\n\n# Labels and title\nax.set_xlabel(\"Motor axis 0\")\nax.set_ylabel(\"Motor axis 1\")\nax.set_zlabel(\"Motor axis 2\")\nax.set_title(\"ðŸš€ 3D Scatter Plot: Motor Axes\")\n\n# Show plot\nplt.show()\n","metadata":{"trusted":true,"id":"4ac6a7b3-1d49-4e2c-a2db-89fea8509a6a","executionInfo":{"status":"aborted","timestamp":1765676577794,"user_tz":360,"elapsed":36300,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract data\nx = train_df['Array shape (axis 0)']\ny = train_df['Array shape (axis 1)']\nz = train_df['Array shape (axis 2)']\ncolors = train_df['Number of motors']\n\n# Create 3D plot\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\n\n# Scatter plot\nsc = ax.scatter(x, y, z, c=colors, cmap=\"magma\", s=50, alpha=0.85)\n\n# Color bar\ncbar = plt.colorbar(sc, ax=ax)\ncbar.set_label('Number of motors')\n\n# Labels and title\nax.set_xlabel(\"Array shape (axis 0)\")\nax.set_ylabel(\"Array shape (axis 1)\")\nax.set_zlabel(\"Array shape (axis 2)\")\nax.set_title(\"ðŸ§¬ 3D Scatter Plot: Tomogram Shapes\")\n\n# Show plot\nplt.show()\n","metadata":{"papermill":{"duration":0.142092,"end_time":"2025-03-07T10:25:30.227979","exception":false,"start_time":"2025-03-07T10:25:30.085887","status":"completed"},"tags":[],"trusted":true,"id":"e5abafd9","executionInfo":{"status":"aborted","timestamp":1765676577797,"user_tz":360,"elapsed":36297,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### # Show descriptive statistics","metadata":{"id":"9737f6eb-8954-4f0b-a3d0-0847662814a5"}},{"cell_type":"code","source":"display(train_df.describe())","metadata":{"papermill":{"duration":1.961473,"end_time":"2025-03-07T10:25:32.222722","exception":false,"start_time":"2025-03-07T10:25:30.261249","status":"completed"},"tags":[],"trusted":true,"id":"ce202320","executionInfo":{"status":"aborted","timestamp":1765676577800,"user_tz":360,"elapsed":36295,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Analyze Correlation\n","metadata":{"papermill":{"duration":0.013441,"end_time":"2025-03-07T10:25:32.249757","exception":false,"start_time":"2025-03-07T10:25:32.236316","status":"completed"},"tags":[],"id":"eef09f82"}},{"cell_type":"code","source":"# Set figure size and background color\nplt.figure(figsize=(9, 5), facecolor=\"white\")\n\n# Generate the heatmap\nsns.heatmap(\n    data=train_df.corr(numeric_only=True),\n    annot=True,\n    fmt=\".2f\"\n)\n\n# Add a title\nplt.title(\"Correlation Heatmap\")\n\n# Display the plot\nplt.show()","metadata":{"papermill":{"duration":0.438725,"end_time":"2025-03-07T10:25:32.701711","exception":false,"start_time":"2025-03-07T10:25:32.262986","status":"completed"},"tags":[],"trusted":true,"id":"1c5d2fd9","executionInfo":{"status":"aborted","timestamp":1765676577802,"user_tz":360,"elapsed":36293,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## look on images in the dataset","metadata":{"papermill":{"duration":0.015141,"end_time":"2025-03-07T10:25:32.733117","exception":false,"start_time":"2025-03-07T10:25:32.717976","status":"completed"},"tags":[],"id":"9a06a884"}},{"cell_type":"code","source":"# Define parameters\npath = \"/kaggle/input/byu-locating-bacterial-flagellar-motors-2025/train/tomo_00e463\"\nn_images = 9\nis_random = True\nfigsize = (12, 12)\n\n# Load image names\nimage_names = os.listdir(path)\n\n# Handle case where directory is empty\nif not image_names:\n    print(\"No images found in the directory.\")\nelse:\n    # Select images (random or sequential)\n    if is_random:\n        image_names = random.sample(image_names, min(len(image_names), n_images))\n    else:\n        image_names = image_names[:n_images]\n\n    # Define grid size\n    w = int(math.sqrt(n_images))\n    h = math.ceil(n_images / w)\n\n    # Create figure\n    plt.figure(figsize=figsize)\n\n    for ind, image_name in enumerate(image_names):\n        img_path = os.path.join(path, image_name)\n        img = cv2.imread(img_path)\n\n        if img is None:\n            continue  # Skip if image is unreadable\n\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        plt.subplot(h, w, ind + 1)\n        plt.imshow(img)\n        plt.xticks([])\n        plt.yticks([])\n\n    # Add title\n    plt.suptitle(\"Sample Tomogram Images\")\n    plt.show()\n","metadata":{"papermill":{"duration":1.663832,"end_time":"2025-03-07T10:25:40.739806","exception":false,"start_time":"2025-03-07T10:25:39.075974","status":"completed"},"tags":[],"trusted":true,"id":"9c868ffd","executionInfo":{"status":"aborted","timestamp":1765676577850,"user_tz":360,"elapsed":36337,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model used YOLO8\n","metadata":{"papermill":{"duration":0.088395,"end_time":"2025-03-07T10:25:42.468027","exception":false,"start_time":"2025-03-07T10:25:42.379632","status":"completed"},"tags":[],"id":"224a387c"}},{"cell_type":"code","source":"from tqdm import tqdm\n\n# Define dataset paths\ndata_path = \"/kaggle/input/byu-locating-bacterial-flagellar-motors-2025/\"\ntrain_dir = os.path.join(data_path, \"train\")\n\n# YOLO dataset structure\nyolo_dir = \"/kaggle/working/yolo_dataset\"\nyolo_img_train, yolo_img_val = os.path.join(yolo_dir, \"images/train\"), os.path.join(yolo_dir, \"images/val\")\nyolo_lbl_train, yolo_lbl_val = os.path.join(yolo_dir, \"labels/train\"), os.path.join(yolo_dir, \"labels/val\")\n\n# Create required directories\nfor path in [yolo_img_train, yolo_img_val, yolo_lbl_train, yolo_lbl_val]:\n    os.makedirs(path, exist_ok=True)\n\n# Constants\nTRUST = 6  # Number of slices above and below center slice\nBOX_SIZE = 28  # Bounding box size in pixels\nTRAIN_SPLIT = 0.8  # Train-validation split ratio\n\ndef normalize_slice(slice_data):\n    \"\"\"\n    Normalize image slice using percentile-based contrast enhancement.\n    \"\"\"\n    p2, p98 = np.percentile(slice_data, [2, 98])\n    return np.uint8(255 * np.clip((slice_data - p2) / (p98 - p2), 0, 1))\n\ndef process_tomograms(tomo_ids, img_dir, lbl_dir, labels_df, trust, set_name):\n    \"\"\"\n    Process tomograms to extract image slices and generate YOLO labels.\n    \"\"\"\n    motor_data = labels_df[labels_df['tomo_id'].isin(tomo_ids)]\n    motor_count = []\n\n    for _, motor in motor_data.iterrows():\n        if pd.isna(motor['Motor axis 0']):\n            continue\n        motor_count.append(\n            (motor['tomo_id'], *map(int, [motor['Motor axis 0'], motor['Motor axis 1'], motor['Motor axis 2'], motor['Array shape (axis 0)']]))\n        )\n\n    print(f\"Processing {len(motor_count)} motors for {set_name}...\")\n    processed = 0\n\n    for tomo_id, z_center, y_center, x_center, z_max in tqdm(motor_count, desc=f\"{set_name} data\"):\n        for z in range(max(0, z_center - trust), min(z_max - 1, z_center + trust) + 1):\n            slice_file = f\"slice_{z:04d}.jpg\"\n            src_path = os.path.join(train_dir, tomo_id, slice_file)\n\n            if not os.path.exists(src_path):\n                continue  # Skip missing slices\n\n            img = np.array(Image.open(src_path))\n            normalized = normalize_slice(img)\n            dest_name = f\"{tomo_id}_z{z:04d}_y{y_center:04d}_x{x_center:04d}.jpg\"\n            Image.fromarray(normalized).save(os.path.join(img_dir, dest_name))\n\n            # Generate YOLO bounding box annotation\n            img_w, img_h = img.shape[1], img.shape[0]\n            label_path = os.path.join(lbl_dir, dest_name.replace('.jpg', '.txt'))\n            with open(label_path, 'w') as f:\n                f.write(f\"0 {x_center/img_w} {y_center/img_h} {BOX_SIZE/img_w} {BOX_SIZE/img_h}\\n\")\n\n            processed += 1\n    return processed, len(motor_count)\n\ndef prepare_yolo_dataset():\n    \"\"\"\n    Prepare the YOLO dataset by extracting slices and generating labels.\n    \"\"\"\n    labels_df = pd.read_csv(os.path.join(data_path, \"train_labels.csv\"))\n    tomo_ids = labels_df[labels_df['Number of motors'] > 0]['tomo_id'].unique()\n\n    # Train-validation split\n    np.random.shuffle(tomo_ids)\n    split_idx = int(len(tomo_ids) * TRAIN_SPLIT)\n    train_tomos, val_tomos = tomo_ids[:split_idx], tomo_ids[split_idx:]\n\n    train_slices, train_motors = process_tomograms(train_tomos, yolo_img_train, yolo_lbl_train, labels_df, TRUST, \"Train\")\n    val_slices, val_motors = process_tomograms(val_tomos, yolo_img_val, yolo_lbl_val, labels_df, TRUST, \"Validation\")\n\n    # Generate YAML config for YOLO\n    with open(os.path.join(yolo_dir, 'dataset.yaml'), 'w') as f:\n        yaml.dump({'path': yolo_dir, 'train': 'images/train', 'val': 'images/val', 'names': {0: 'motor'}}, f)\n\n    print(\"\\nDataset Preparation Complete!\")\n    print(f\"- Train: {len(train_tomos)} tomograms, {train_motors} motors, {train_slices} slices\")\n    print(f\"- Validation: {len(val_tomos)} tomograms, {val_motors} motors, {val_slices} slices\")\n    print(f\"- Dataset directory: {yolo_dir}\")\n\n# Run preprocessing\nprepare_yolo_dataset()\n","metadata":{"papermill":{"duration":172.349279,"end_time":"2025-03-07T10:28:34.903887","exception":false,"start_time":"2025-03-07T10:25:42.554608","status":"completed"},"tags":[],"trusted":true,"id":"a2e54c90","executionInfo":{"status":"aborted","timestamp":1765676577862,"user_tz":360,"elapsed":36345,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize yolo data","metadata":{"papermill":{"duration":0.089574,"end_time":"2025-03-07T10:28:35.083940","exception":false,"start_time":"2025-03-07T10:28:34.994366","status":"completed"},"tags":[],"id":"19968df7"}},{"cell_type":"code","source":"# Directories for training images and labels\nyolo_dataset_dir=\"/kaggle/working/yolo_dataset\"\ntrain_images_dir = os.path.join(yolo_dataset_dir, \"images\", \"train\")\ntrain_labels_dir = os.path.join(yolo_dataset_dir, \"labels\", \"train\")\n\ndef display_random_samples(num_samples=4):\n    \"\"\"\n    Displays a specified number of random training images with YOLO annotations.\n\n    Args:\n        num_samples (int): The number of random images to display.\n    \"\"\"\n    # Collect all image files (supports multiple formats)\n    image_files = []\n    for ext in ['*.jpg', '*.jpeg', '*.png']:\n        image_files.extend(glob.glob(os.path.join(train_images_dir, \"**\", ext), recursive=True))\n\n    if not image_files:\n        print(\"No images found in the training directory.\")\n        return\n\n    num_samples = min(num_samples, len(image_files))\n    selected_images = random.sample(image_files, num_samples)\n\n    # Create subplots\n    rows = int(np.ceil(num_samples / 2))\n    cols = min(num_samples, 2)\n    fig, axes = plt.subplots(rows, cols, figsize=(14, 5 * rows))\n\n    axes = axes.flatten() if num_samples > 1 else np.array([axes])\n\n    for idx, img_path in enumerate(selected_images):\n        try:\n            # Get the corresponding label file\n            relative_path = os.path.relpath(img_path, train_images_dir)\n            label_path = os.path.join(train_labels_dir, os.path.splitext(relative_path)[0] + '.txt')\n\n            # Load and normalize the image\n            img = Image.open(img_path)\n            img_width, img_height = img.size\n            img_array = np.array(img)\n            p2, p98 = np.percentile(img_array, 2), np.percentile(img_array, 98)\n            img_normalized = np.clip(img_array, p2, p98)\n            img_normalized = 255 * (img_normalized - p2) / (p98 - p2)\n            img_normalized = Image.fromarray(np.uint8(img_normalized))\n\n            # Prepare image for annotation\n            img_rgb = img_normalized.convert('RGB')\n            overlay = Image.new('RGBA', img_rgb.size, (0, 0, 0, 0))\n            draw = ImageDraw.Draw(overlay)\n\n            # Load YOLO annotations if they exist\n            annotations = []\n            if os.path.exists(label_path):\n                with open(label_path, 'r') as label_file:\n                    for line in label_file:\n                        class_id, x_center, y_center, width, height = map(float, line.strip().split())\n                        x_center, y_center = x_center * img_width, y_center * img_height\n                        width, height = width * img_width, height * img_height\n                        annotations.append({'class_id': int(class_id), 'x_center': x_center, 'y_center': y_center, 'width': width, 'height': height})\n\n            # Draw annotations\n            for ann in annotations:\n                x1 = max(0, int(ann['x_center'] - ann['width'] / 2))\n                y1 = max(0, int(ann['y_center'] - ann['height'] / 2))\n                x2 = min(img_width, int(ann['x_center'] + ann['width'] / 2))\n                y2 = min(img_height, int(ann['y_center'] + ann['height'] / 2))\n                draw.rectangle([x1, y1, x2, y2], fill=(255, 0, 0, 64), outline=(255, 0, 0, 200))\n                draw.text((x1, y1 - 10), f\"Class {ann['class_id']}\", fill=(255, 0, 0, 255))\n\n            # If no annotations, indicate it on the image\n            if not annotations:\n                draw.text((10, 10), \"No annotations found\", fill=(255, 0, 0, 255))\n\n            # Composite and show image\n            img_final = Image.alpha_composite(img_rgb.convert('RGBA'), overlay).convert('RGB')\n            axes[idx].imshow(np.array(img_final))\n            axes[idx].set_title(f\"Image: {os.path.basename(img_path)}\\nAnnotations: {len(annotations)}\")\n            axes[idx].axis('off')\n\n        except Exception as e:\n            print(f\"Error processing image {img_path}: {e}\")\n            axes[idx].text(0.5, 0.5, f\"Error loading: {os.path.basename(img_path)}\", ha='center', va='center')\n            axes[idx].axis('off')\n\n    # Hide any extra axes\n    for i in range(idx + 1, len(axes)):\n        axes[i].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n    print(f\"Displayed {num_samples} random images with YOLO annotations.\")\n\n# Call the function to visualize random training samples\ndisplay_random_samples(4)\n","metadata":{"papermill":{"duration":1.717906,"end_time":"2025-03-07T10:28:36.891635","exception":false,"start_time":"2025-03-07T10:28:35.173729","status":"completed"},"tags":[],"trusted":true,"id":"140d75b9","executionInfo":{"status":"aborted","timestamp":1765676577864,"user_tz":360,"elapsed":36342,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Training\n","metadata":{"papermill":{"duration":0.125352,"end_time":"2025-03-07T10:28:37.142507","exception":false,"start_time":"2025-03-07T10:28:37.017155","status":"completed"},"tags":[],"id":"32b08ba8"}},{"cell_type":"code","source":"# yaml fixing\ndef fix_yaml_paths(yaml_path):\n    \"\"\"Fix YAML paths to match the Kaggle directories.\"\"\"\n    with open(yaml_path, 'r') as f:\n        yaml_data = yaml.safe_load(f)\n\n    yaml_data['path'] = yolo_dataset_dir  # Update path\n    fixed_yaml_path = \"/kaggle/working/fixed_dataset.yaml\"\n\n    with open(fixed_yaml_path, 'w') as f:\n        yaml.dump(yaml_data, f)\n\n    print(f\"Fixed YAML created at {fixed_yaml_path}\")\n    return fixed_yaml_path\n","metadata":{"papermill":{"duration":0.139181,"end_time":"2025-03-07T10:28:37.913972","exception":false,"start_time":"2025-03-07T10:28:37.774791","status":"completed"},"tags":[],"trusted":true,"id":"e1f102de","executionInfo":{"status":"aborted","timestamp":1765676577867,"user_tz":360,"elapsed":36342,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set random seeds for reproducibility\nnp.random.seed(00)\nrandom.seed(42)\ntorch.manual_seed(42)\n\n# Define paths for the Kaggle environment\nyolo_dataset_dir = \"/kaggle/working/yolo_dataset\"\nyolo_weights_dir = \"/kaggle/working/yolo_weights\"\nyolo_pretrained_weights = \"/kaggle/input/ultralytics-for-offline-install/yolov8n.pt\"  # Pre-downloaded weights\n\n# Create the weights directory if it does not exist\nos.makedirs(yolo_weights_dir, exist_ok=True)","metadata":{"papermill":{"duration":0.132762,"end_time":"2025-03-07T10:28:37.398719","exception":false,"start_time":"2025-03-07T10:28:37.265957","status":"completed"},"tags":[],"trusted":true,"id":"f24722a8","executionInfo":{"status":"aborted","timestamp":1765676577866,"user_tz":360,"elapsed":36343,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_curves(run_dir):\n    \"\"\"Plot DFL loss curves and highlight the best model.\"\"\"\n    results_csv = os.path.join(run_dir, 'results.csv')\n    if not os.path.exists(results_csv):\n        print(f\"Results file not found: {results_csv}\")\n        return\n\n    df = pd.read_csv(results_csv)\n    train_dfl_col = next((col for col in df.columns if 'train/dfl_loss' in col), None)\n    val_dfl_col = next((col for col in df.columns if 'val/dfl_loss' in col), None)\n\n    if not train_dfl_col or not val_dfl_col:\n        print(\"DFL loss columns not found.\")\n        return\n\n    best_epoch = df[val_dfl_col].idxmin()\n    best_val_loss = df.loc[best_epoch, val_dfl_col]\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['epoch'], df[train_dfl_col], label='Train DFL Loss')\n    plt.plot(df['epoch'], df[val_dfl_col], label='Validation DFL Loss')\n    plt.axvline(x=df.loc[best_epoch, 'epoch'], color='r', linestyle='--',\n                label=f'Best Model (Epoch {df.loc[best_epoch, \"epoch\"]}, Loss: {best_val_loss:.4f})')\n    plt.xlabel('Epoch')\n    plt.ylabel('DFL Loss')\n    plt.title('Training and Validation DFL Loss')\n    plt.legend()\n    plt.grid(True, linestyle='--', alpha=0.7)\n\n    plot_path = os.path.join(run_dir, 'dfl_loss_curve.png')\n    plt.savefig(plot_path)\n    plt.savefig('/kaggle/working/dfl_loss_curve.png')\n    print(f\"Loss curve saved to {plot_path}\")\n    plt.close()\n\n    return best_epoch, best_val_loss","metadata":{"papermill":{"duration":0.175124,"end_time":"2025-03-07T10:28:38.475366","exception":false,"start_time":"2025-03-07T10:28:38.300242","status":"completed"},"tags":[],"trusted":true,"id":"3a79a5c2","executionInfo":{"status":"aborted","timestamp":1765676577870,"user_tz":360,"elapsed":36345,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\ndef train_yolo_model(yaml_path, pretrained_weights_path, epochs=30, batch_size=16, img_size=640):\n    \"\"\"\n    Train a YOLO model on the prepared dataset.\n\n    Args:\n        yaml_path (str): Path to the dataset YAML file.\n        pretrained_weights_path (str): Path to pre-downloaded weights file.\n        epochs (int): Number of training epochs.\n        batch_size (int): Batch size for training.\n        img_size (int): Image size for training.\n    \"\"\"\n    print(f\"Loading pre-trained weights from: {pretrained_weights_path}\")\n    assert os.path.exists(pretrained_weights_path), f\"Weights not found: {pretrained_weights_path}\"\n    assert pretrained_weights_path.endswith(\".pt\"), f\"Expected a .pt file, got: {pretrained_weights_path}\"\n    model = YOLO(pretrained_weights_path)\n\n    results = model.train(\n        data=yaml_path,\n        epochs=epochs,\n        batch=batch_size,\n        imgsz=img_size,\n        project=yolo_weights_dir,\n        name='motor_detector',\n        exist_ok=True,\n        patience=5,\n        save_period=5,\n        val=True,\n        verbose=True\n    )\n\n    run_dir = os.path.join(yolo_weights_dir, 'motor_detector')\n    best_epoch_info = plot_curves(run_dir)\n    if best_epoch_info:\n        best_epoch, best_val_loss = best_epoch_info\n        print(f\"\\nBest model found at epoch {best_epoch} with validation DFL loss: {best_val_loss:.4f}\")\n\n    return model, results","metadata":{"papermill":{"duration":0.146204,"end_time":"2025-03-07T10:28:39.236696","exception":false,"start_time":"2025-03-07T10:28:39.090492","status":"completed"},"tags":[],"trusted":true,"id":"c2e607d4","executionInfo":{"status":"aborted","timestamp":1765676577872,"user_tz":360,"elapsed":36346,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef predict_on_samples(model, num_samples=4):\n    \"\"\"\n    Run predictions on random validation samples and display results.\n\n    Args:\n        model: Trained YOLO model.\n        num_samples (int): Number of random samples to test.\n    \"\"\"\n    val_dir = os.path.join(yolo_dataset_dir, 'images', 'val')\n    if not os.path.exists(val_dir):\n        print(f\"Validation directory not found at {val_dir}\")\n        val_dir = os.path.join(yolo_dataset_dir, 'images', 'train')\n        print(f\"Using train directory for predictions instead: {val_dir}\")\n\n    if not os.path.exists(val_dir):\n        print(\"No images directory found for predictions\")\n        return\n\n    val_images = os.listdir(val_dir)\n    if len(val_images) == 0:\n        print(\"No images found for prediction\")\n        return\n\n    num_samples = min(num_samples, len(val_images))\n    samples = random.sample(val_images, num_samples)\n\n    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n    axes = axes.flatten()\n\n    for i, img_file in enumerate(samples):\n        if i >= len(axes):\n            break\n\n        img_path = os.path.join(val_dir, img_file)\n        results = model.predict(img_path, conf=0.25)[0]\n        img = Image.open(img_path)\n        axes[i].imshow(np.array(img), cmap='gray')\n\n        # Draw ground truth box if available (extracted from filename)\n        try:\n            parts = img_file.split('_')\n            y_part = [p for p in parts if p.startswith('y')]\n            x_part = [p for p in parts if p.startswith('x')]\n            if y_part and x_part:\n                y_gt = int(y_part[0][1:])\n                x_gt = int(x_part[0][1:].split('.')[0])\n                box_size = 28\n                rect_gt = Rectangle((x_gt - box_size//2, y_gt - box_size//2), box_size, box_size,\n                                      linewidth=1, edgecolor='g', facecolor='none')\n                axes[i].add_patch(rect_gt)\n        except:\n            pass\n\n        if len(results.boxes) > 0:\n            boxes = results.boxes.xyxy.cpu().numpy()\n            confs = results.boxes.conf.cpu().numpy()\n            for box, conf in zip(boxes, confs):\n                x1, y1, x2, y2 = box\n                rect_pred = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')\n                axes[i].add_patch(rect_pred)\n                axes[i].text(x1, y1-5, f'{conf:.2f}', color='red')\n\n        axes[i].set_title(f\"Image: {img_file}\\nGT (green) vs Pred (red)\")\n\n    plt.tight_layout()\n    plt.savefig(os.path.join('/kaggle/working', 'predictions.png'))\n    plt.show()","metadata":{"papermill":{"duration":0.138047,"end_time":"2025-03-07T10:28:39.497383","exception":false,"start_time":"2025-03-07T10:28:39.359336","status":"completed"},"tags":[],"trusted":true,"id":"42719291","executionInfo":{"status":"aborted","timestamp":1765676577874,"user_tz":360,"elapsed":36348,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_dataset():\n    \"\"\"\n    Check if the dataset exists and create/fix a proper YAML file for training.\n\n    Returns:\n        str: Path to the YAML file to use for training.\n    \"\"\"\n    train_images_dir = os.path.join(yolo_dataset_dir, 'images', 'train')\n    val_images_dir = os.path.join(yolo_dataset_dir, 'images', 'val')\n    train_labels_dir = os.path.join(yolo_dataset_dir, 'labels', 'train')\n    val_labels_dir = os.path.join(yolo_dataset_dir, 'labels', 'val')\n\n    print(f\"Directory status:\")\n    print(f\"- Train images exists: {os.path.exists(train_images_dir)}\")\n    print(f\"- Val images exists: {os.path.exists(val_images_dir)}\")\n    print(f\"- Train labels exists: {os.path.exists(train_labels_dir)}\")\n    print(f\"- Val labels exists: {os.path.exists(val_labels_dir)}\")\n\n    original_yaml_path = os.path.join(yolo_dataset_dir, 'dataset.yaml')\n    if os.path.exists(original_yaml_path):\n        print(f\"Found original dataset.yaml at {original_yaml_path}\")\n        return fix_yaml_paths(original_yaml_path)\n    else:\n        print(\"Original dataset.yaml not found, creating a new one\")\n        yaml_data = {\n            'path': yolo_dataset_dir,\n            'train': 'images/train',\n            'val': 'images/train' if not os.path.exists(val_images_dir) else 'images/val',\n            'names': {0: 'motor'}\n        }\n        new_yaml_path = \"/kaggle/working/dataset.yaml\"\n        with open(new_yaml_path, 'w') as f:\n            yaml.dump(yaml_data, f)\n        print(f\"Created new YAML at {new_yaml_path}\")\n        return new_yaml_path\n","metadata":{"papermill":{"duration":1152.29292,"end_time":"2025-03-07T10:47:52.159537","exception":false,"start_time":"2025-03-07T10:28:39.866617","status":"completed"},"tags":[],"trusted":true,"id":"1aaff0f6","executionInfo":{"status":"aborted","timestamp":1765676577876,"user_tz":360,"elapsed":36349,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil, os\n\nsrc = \"/kaggle/input/ultralytics-for-offline-install/yolov8n.pt\"\ndst = \"/kaggle/working/yolov8n.pt\"\n\nassert os.path.exists(src), f\"Missing weights at {src}\"\nshutil.copy(src, dst)\nprint(\"Copied to:\", dst, \"size:\", os.path.getsize(dst))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"yaml_path = prepare_dataset()\nprint(f\"Using YAML file: {yaml_path}\")\nwith open(yaml_path, 'r') as f:\n    print(f\"YAML contents:\\n{f.read()}\")\n\nmodel, results = train_yolo_model(\n    yaml_path,\n    pretrained_weights_path=yolo_pretrained_weights,\n    epochs=30\n)\n\nprint(\"predictions\")\npredict_on_samples(model, num_samples=4)\n","metadata":{"trusted":true,"id":"83d713a5-87be-492f-b719-46ce0aa5b759","executionInfo":{"status":"aborted","timestamp":1765676577878,"user_tz":360,"elapsed":36348,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## create submission","metadata":{"id":"f6dcec1d-3078-4285-9d77-33a34848fe78"}},{"cell_type":"code","source":"# random seed\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Define paths for the test data and submission\ndata_path = \"/kaggle/input/byu-locating-bacterial-flagellar-motors-2025/\"\ntest_dir = os.path.join(data_path, \"test\")\nsubmission_path = \"/kaggle/working/submission.csv\"\n\n# Path to the best trained model (adjust if necessary)\nmodel_path = \"/kaggle/working/yolo_weights/motor_detector/weights/best.pt\"\n\n# Define detection and processing parameters\nCONFIDENCE_THRESHOLD = 0.45\nMAX_DETECTIONS_PER_TOMO = 3\nNMS_IOU_THRESHOLD = 0.2\nCONCENTRATION = 1  # Process a fraction of slices for fast submission\n\n# GPU profiling context manager for timing\nclass GPUProfiler:\n    def __init__(self, name):\n        self.name = name\n        self.start_time = None\n\n    def __enter__(self):\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        self.start_time = time.time()\n        return self\n\n    def __exit__(self, *args):\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        elapsed = time.time() - self.start_time\n        print(f\"[PROFILE] {self.name}: {elapsed:.3f}s\")\n\n# Set device and dynamic batch size\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = 8\nif device.startswith('cuda'):\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"Using GPU: {gpu_name} with {gpu_mem:.2f} GB memory\")\n    free_mem = gpu_mem - torch.cuda.memory_allocated(0) / 1e9\n    BATCH_SIZE = max(8, min(32, int(free_mem * 4)))\n    print(f\"Dynamic batch size set to {BATCH_SIZE} based on {free_mem:.2f}GB free memory\")\nelse:\n    print(\"GPU not available, using CPU\")\n    BATCH_SIZE = 4","metadata":{"papermill":{"duration":0.60338,"end_time":"2025-03-07T10:47:55.864258","exception":false,"start_time":"2025-03-07T10:47:55.260878","status":"completed"},"tags":[],"trusted":true,"id":"431712d8","executionInfo":{"status":"aborted","timestamp":1765676577880,"user_tz":360,"elapsed":36346,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference Creation","metadata":{"papermill":{"duration":0.58907,"end_time":"2025-03-07T10:47:57.081699","exception":false,"start_time":"2025-03-07T10:47:56.492629","status":"completed"},"tags":[],"id":"5687fb43"}},{"cell_type":"code","source":"def normalize_slice(slice_data):\n    \"\"\"\n    Normalize slice data using the 2nd and 98th percentiles.\n    \"\"\"\n    p2 = np.percentile(slice_data, 2)\n    p98 = np.percentile(slice_data, 98)\n    clipped_data = np.clip(slice_data, p2, p98)\n    normalized = 255 * (clipped_data - p2) / (p98 - p2)\n    return np.uint8(normalized)\n\ndef preload_image_batch(file_paths):\n    \"\"\"Preload a batch of images to CPU memory.\"\"\"\n    images = []\n    for path in file_paths:\n        img = cv2.imread(path)\n        if img is None:\n            img = np.array(Image.open(path))\n        images.append(img)\n    return images\n\ndef perform_3d_nms(detections, iou_threshold):\n    \"\"\"\n    Perform 3D Non-Maximum Suppression on detections to merge nearby motors.\n    \"\"\"\n    if not detections:\n        return []\n\n    detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)\n    final_detections = []\n    def distance_3d(d1, d2):\n        return np.sqrt((d1['z'] - d2['z'])**2 + (d1['y'] - d2['y'])**2 + (d1['x'] - d2['x'])**2)\n\n    box_size = 28\n    distance_threshold = box_size * iou_threshold\n\n    while detections:\n        best_detection = detections.pop(0)\n        final_detections.append(best_detection)\n        detections = [d for d in detections if distance_3d(d, best_detection) > distance_threshold]\n\n    return final_detections\n\ndef process_tomogram(tomo_id, model, index=0, total=1):\n    \"\"\"\n    Process a single tomogram and return the most confident motor detection.\n    \"\"\"\n    print(f\"Processing tomogram {tomo_id} ({index}/{total})\")\n    tomo_dir = os.path.join(test_dir, tomo_id)\n    slice_files = sorted([f for f in os.listdir(tomo_dir) if f.endswith('.jpg')])\n\n    selected_indices = np.linspace(0, len(slice_files)-1, int(len(slice_files) * CONCENTRATION))\n    selected_indices = np.round(selected_indices).astype(int)\n    slice_files = [slice_files[i] for i in selected_indices]\n\n    print(f\"Processing {len(slice_files)} out of {len(os.listdir(tomo_dir))} slices (CONCENTRATION={CONCENTRATION})\")\n    all_detections = []\n\n    if device.startswith('cuda'):\n        streams = [torch.cuda.Stream() for _ in range(min(4, BATCH_SIZE))]\n    else:\n        streams = [None]\n\n    next_batch_thread = None\n    next_batch_images = None\n\n    for batch_start in range(0, len(slice_files), BATCH_SIZE):\n        if next_batch_thread is not None:\n            next_batch_thread.join()\n            next_batch_images = None\n\n        batch_end = min(batch_start + BATCH_SIZE, len(slice_files))\n        batch_files = slice_files[batch_start:batch_end]\n\n        next_batch_start = batch_end\n        next_batch_end = min(next_batch_start + BATCH_SIZE, len(slice_files))\n        next_batch_files = slice_files[next_batch_start:next_batch_end] if next_batch_start < len(slice_files) else []\n        if next_batch_files:\n            next_batch_paths = [os.path.join(tomo_dir, f) for f in next_batch_files]\n            next_batch_thread = threading.Thread(target=preload_image_batch, args=(next_batch_paths,))\n            next_batch_thread.start()\n        else:\n            next_batch_thread = None\n\n        sub_batches = np.array_split(batch_files, len(streams))\n        for i, sub_batch in enumerate(sub_batches):\n            if len(sub_batch) == 0:\n                continue\n            stream = streams[i % len(streams)]\n            with torch.cuda.stream(stream) if stream and device.startswith('cuda') else nullcontext():\n                sub_batch_paths = [os.path.join(tomo_dir, slice_file) for slice_file in sub_batch]\n                sub_batch_slice_nums = [int(slice_file.split('_')[1].split('.')[0]) for slice_file in sub_batch]\n                with GPUProfiler(f\"Inference batch {i+1}/{len(sub_batches)}\"):\n                    sub_results = model(sub_batch_paths, verbose=False)\n                for j, result in enumerate(sub_results):\n                    if len(result.boxes) > 0:\n                        for box_idx, confidence in enumerate(result.boxes.conf):\n                            if confidence >= CONFIDENCE_THRESHOLD:\n                                x1, y1, x2, y2 = result.boxes.xyxy[box_idx].cpu().numpy()\n                                x_center = (x1 + x2) / 2\n                                y_center = (y1 + y2) / 2\n                                all_detections.append({\n                                    'z': round(sub_batch_slice_nums[j]),\n                                    'y': round(y_center),\n                                    'x': round(x_center),\n                                    'confidence': float(confidence)\n                                })\n        if device.startswith('cuda'):\n            torch.cuda.synchronize()\n\n    if next_batch_thread is not None:\n        next_batch_thread.join()\n\n    final_detections = perform_3d_nms(all_detections, NMS_IOU_THRESHOLD)\n    final_detections.sort(key=lambda x: x['confidence'], reverse=True)\n\n    if not final_detections:\n        return {'tomo_id': tomo_id, 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n\n    best_detection = final_detections[0]\n    return {\n        'tomo_id': tomo_id,\n        'Motor axis 0': round(best_detection['z']),\n        'Motor axis 1': round(best_detection['y']),\n        'Motor axis 2': round(best_detection['x'])\n    }\n\ndef debug_image_loading(tomo_id):\n    \"\"\"\n    Debug function to test image loading methods.\n    \"\"\"\n    tomo_dir = os.path.join(test_dir, tomo_id)\n    slice_files = sorted([f for f in os.listdir(tomo_dir) if f.endswith('.jpg')])\n    if not slice_files:\n        print(f\"No image files found in {tomo_dir}\")\n        return\n\n    print(f\"Found {len(slice_files)} image files in {tomo_dir}\")\n    sample_file = slice_files[len(slice_files)//2]\n    img_path = os.path.join(tomo_dir, sample_file)\n\n    try:\n        img_pil = Image.open(img_path)\n        print(f\"PIL Image shape: {np.array(img_pil).shape}, dtype: {np.array(img_pil).dtype}\")\n        img_cv2 = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        print(f\"OpenCV Image shape: {img_cv2.shape}, dtype: {img_cv2.dtype}\")\n        img_rgb = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n        print(f\"OpenCV RGB Image shape: {img_rgb.shape}, dtype: {img_rgb.dtype}\")\n        print(\"Image loading successful!\")\n    except Exception as e:\n        print(f\"Error loading image {img_path}: {e}\")\n\n    try:\n        test_model = YOLO(model_path)\n        test_results = test_model([img_path], verbose=False)\n        print(\"YOLO model successfully processed the test image\")\n    except Exception as e:\n        print(f\"Error with YOLO processing: {e}\")","metadata":{"papermill":{"duration":0.673032,"end_time":"2025-03-07T10:47:58.334828","exception":false,"start_time":"2025-03-07T10:47:57.661796","status":"completed"},"tags":[],"trusted":true,"id":"69e496a2","executionInfo":{"status":"aborted","timestamp":1765676577882,"user_tz":360,"elapsed":36347,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Generate CSV","metadata":{"papermill":{"duration":0.619626,"end_time":"2025-03-07T10:47:59.532925","exception":false,"start_time":"2025-03-07T10:47:58.913299","status":"completed"},"tags":[],"id":"1ff1a6e5"}},{"cell_type":"code","source":"test_tomos = sorted([d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))])\ntotal_tomos = len(test_tomos)\nprint(f\"Total tomograms in test:{total_tomos}\")\n\nif test_tomos:\n    debug_image_loading(test_tomos[0])\n\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint(f\"Loading YOLO model from {model_path}\")\nmodel = YOLO(model_path)\nmodel.to(device)\nif device.startswith('cuda'):\n    model.fuse()\n    if torch.cuda.get_device_capability(0)[0] >= 7:\n        model.model.half()\n        print(\"Using half precision (FP16) for inference\")\n\nresults = []\nmotors_found = 0\n\nwith ThreadPoolExecutor(max_workers=1) as executor:\n    future_to_tomo = {}\n    for i, tomo_id in enumerate(test_tomos, 1):\n        future = executor.submit(process_tomogram, tomo_id, model, i, total_tomos)\n        future_to_tomo[future] = tomo_id\n\n    for future in future_to_tomo:\n        tomo_id = future_to_tomo[future]\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            result = future.result()\n            results.append(result)\n            has_motor = not pd.isna(result['Motor axis 0'])\n            if has_motor:\n                motors_found += 1\n                print(f\"Motor found in {tomo_id} at position: z={result['Motor axis 0']}, y={result['Motor axis 1']}, x={result['Motor axis 2']}\")\n            else:\n                print(f\"No motor detected in {tomo_id}\")\n            print(f\"Current detection rate: {motors_found}/{len(results)} ({motors_found/len(results)*100:.1f}%)\")\n        except Exception as e:\n            print(f\"Error processing {tomo_id}: {e}\")\n            results.append({'tomo_id': tomo_id, 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1})\n\nsubmission_df = pd.DataFrame(results)\nsubmission_df = submission_df[['tomo_id', 'Motor axis 0', 'Motor axis 1', 'Motor axis 2']]\nsubmission_df.to_csv(submission_path, index=False)\n","metadata":{"papermill":{"duration":0.591723,"end_time":"2025-03-07T10:48:00.754679","exception":false,"start_time":"2025-03-07T10:48:00.162956","status":"completed"},"tags":[],"trusted":true,"id":"02d50272","executionInfo":{"status":"aborted","timestamp":1765676578226,"user_tz":360,"elapsed":36688,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## display submission\nimport os\n\n# Kaggle expects this exact filename in the notebook outputs\nsubmission_path = \"/kaggle/working/submission.csv\"\n\nsubmission_df = pd.DataFrame(results)\nsubmission_df = submission_df[['tomo_id', 'Motor axis 0', 'Motor axis 1', 'Motor axis 2']]\n\nsubmission_df.to_csv(submission_path, index=False)\n\nprint(\"Wrote:\", submission_path)\nprint(\"Exists:\", os.path.exists(submission_path))\n\nsubmission_df.head()","metadata":{"papermill":{"duration":0.674078,"end_time":"2025-03-07T10:48:53.399860","exception":false,"start_time":"2025-03-07T10:48:52.725782","status":"completed"},"tags":[],"trusted":true,"id":"30b6686a","executionInfo":{"status":"aborted","timestamp":1765676578229,"user_tz":360,"elapsed":36687,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"id":"afe44afa-9f9e-4dd2-ab5c-6273399a6265","executionInfo":{"status":"aborted","timestamp":1765676578231,"user_tz":360,"elapsed":36688,"user":{"displayName":"jemima egwurube","userId":"02870357230313604668"}}},"outputs":[],"execution_count":null}]}